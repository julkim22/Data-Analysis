# -*- coding: utf-8 -*-
"""7. Sarcopenia Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-bTIqVqxEl5Bvrxxp9cttumbBK01AT1
"""

from google.colab import drive
drive.mount("/content/drive")

cd /content/drive/MyDrive/Colab/

!pip install eli5

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from statsmodels.stats.outliers_influence import variance_inflation_factor
import plotly.graph_objects as go
from imblearn.combine import SMOTETomek
from eli5.sklearn import PermutationImportance
import eli5
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams
from imblearn.under_sampling import *
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import plot_importance
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold, cross_val_score,cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings(action='ignore')
# %matplotlib inline
from imblearn.under_sampling import TomekLinks,OneSidedSelection,RandomUnderSampler

def CVROC(model, X, y):
    cv = StratifiedKFold(n_splits = 5, shuffle=True,random_state = Ran_state)
    classifier = model

    prob = pd.DataFrame(columns=['LR','SVM','RF','XGB','GT'])

    for i, (train, test) in enumerate(cv.split(X, y)):
        LR = classifier[0].fit(X.iloc[train], y.iloc[train])
        RF = classifier[1].fit(X.iloc[train], y.iloc[train])
        XGB = classifier[2].fit(X.iloc[train], y.iloc[train])
        SVM = classifier[3].fit(X.iloc[train], y.iloc[train])
        lrproba = LR.predict_proba(X.iloc[test])[:, -1]
        rfproba = RF.predict_proba(X.iloc[test])[:, -1]
        xgbproba = XGB.predict_proba(X.iloc[test])[:, -1]
        svmproba = SVM.predict_proba(X.iloc[test])[:, -1]
        foldprob = pd.DataFrame(list(zip(lrproba, svmproba, rfproba, xgbproba, y.iloc[test].values)),
                                columns=['LR','SVM','RF','XGB','GT'])
        prob = prob.append(foldprob)
    print(lrproba)
    print(prob)
    prob.to_csv("CVprob-mus.csv")
    lrfpr, lrtpr, _ = roc_curve(list(prob['GT']), list(prob['LR']))
    svmfpr, svmtpr, _ = roc_curve(list(prob['GT']), list(prob['SVM']))
    rffpr, rftpr, _ = roc_curve(list(prob['GT']), list(prob['RF']))
    xgbfpr, xgbtpr, _ = roc_curve(list(prob['GT']), list(prob['XGB']))
    lrauc = auc(lrfpr, lrtpr )
    svmauc = auc(svmfpr, svmtpr)
    rfauc = auc(rffpr, rftpr )
    xgbauc = auc(xgbfpr, xgbtpr)

    plt.figure(dpi=300, facecolor='white')
    rcParams['font.family'] = "Times New Roman"
    rcParams["legend.facecolor"] = 'white'
    rcParams["legend.edgecolor"] = 'white'
    plt.plot(lrfpr, lrtpr,  label="LR CV= %0.2f"  %(round(lrauc , 3)))
    plt.plot(svmfpr, svmtpr, label="SVM CV= %0.2f" %(round(svmauc, 3)))
    plt.plot(rffpr, rftpr,  label="RF CV= %0.2f"  %(round(rfauc , 3)))
    plt.plot(xgbfpr, xgbtpr, label="XGB CV= %0.2f" %(round(xgbauc, 3)))
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('1-specificity', fontsize=18)
    plt.ylabel('sensitivity', fontsize=18)
    plt.legend(loc="lower right", fontsize=12)
    plt.show()

Ran_state = 500

LR = LogisticRegression(C=115.72036673076951, random_state=1818)
RF = RandomForestClassifier(max_depth=171, min_samples_split=3, n_estimators=611,
                        random_state=1818)
XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
               colsample_bynode=1, colsample_bytree=1, eval_metric='auc',
               gamma=0, importance_type='gain',
               interaction_constraints='', learning_rate=0.009979787806885843,
               max_delta_step=0, max_depth=9, min_child_weight=1, missing=-999, n_estimators=425, n_jobs=88,
               num_parallel_tree=1, random_state=1818, reg_alpha=0, reg_lambda=1,
               scale_pos_weight=1, subsample=1, tree_method='exact',
               use_label_encoder=False, validate_parameters=1)
SVM = SVC(C=990.0511805421546, gamma=12.630867909389995, probability=True,
     random_state=1818)

classlist = [LR, RF, XGB, SVM]

df = pd.read_csv('data/Sarcopenia_Data.csv')
df = df.drop(['ID','Sarco1'], 1)
cv = StratifiedKFold(n_splits = 5, shuffle=True,random_state = Ran_state)

data_y = df['Sarco2']
data_x = df.drop(['Sarco2'], 1)

data_y.value_counts()

data_x

"""다중공선성 높은 변수들 제거"""

for i in range(len(data_x.columns)):
    vif = pd.DataFrame()
    vif["features"] = data_x.columns
    vif["VIF Factor"] = [variance_inflation_factor(
        data_x.values, i) for i in range(data_x.shape[1])]
    vif_remove = vif.sort_values(ascending=False, by='VIF Factor')[:]
    if vif_remove.iloc[0][1] > 10:
        data_x = data_x.drop([vif_remove.iloc[0][0]], 1)

vif = pd.DataFrame()
vif["features"] = data_x.columns
vif["VIF Factor"] = [variance_inflation_factor(
    data_x.values, i) for i in range(data_x.shape[1])]
vif_remove = vif.sort_values(ascending=False, by='VIF Factor')[:]
vif_remove

MM = RobustScaler() #정규화
DFSMM = MM.fit_transform(data_x)
data_x = pd.DataFrame(DFSMM, columns=data_x.columns)

data_x, data_y = OneSidedSelection(random_state=Ran_state).fit_resample(data_x, data_y)
data_y.value_counts()

data_y.value_counts()

train_x, test_x, train_y, test_y = train_test_split(
    data_x, data_y, test_size=0.3, random_state=Ran_state,stratify=data_y)

"""==================================================================

학습 및 예측
"""

def create_curve(classlist, train_x, train_y, test_x, test_y):
    MLP = classlist[0]
    RF = classlist[1]
    XGB = classlist[2]
    NB = classlist[3]

    MLP.fit(train_x, train_y)
    RF.fit(train_x, train_y)
    XGB.fit(train_x, train_y)
    NB.fit(train_x, train_y)



    print('==========================================')
    print(train_x.columns)
    print('==========================================')

    lr_pred = MLP.predict(test_x)
    rf_pred = RF.predict(test_x)
    xgb_pred = XGB.predict(test_x)
    svm_pred = NB.predict(test_x)

    print(rf_pred)

    MLP_proba = MLP.predict_proba(test_x)[:, 1]
    rf_proba = RF.predict_proba(test_x)[:, 1]
    xgb_proba = XGB.predict_proba(test_x)[:, 1]
    NB_proba = NB.predict_proba(test_x)[:, 1]
    proba = pd.DataFrame(list(zip(test_y, MLP_proba, NB_proba, rf_proba, xgb_proba)), columns=[
                         'GT', 'LR', 'SVM', 'RF', 'XGB'])
    proba.to_csv("prob-mus.csv")

    fpr_mlp, tpr_mlp, _ = roc_curve(test_y, MLP_proba)
    fpr_rf, tpr_rf, _ = roc_curve(test_y, rf_proba)
    fpr_xgb, tpr_xgb, _ = roc_curve(test_y, xgb_proba)
    fpr_nb, tpr_nb, _ = roc_curve(test_y, NB_proba)


    print('LR')
    tn, fp, fn, tp = confusion_matrix(test_y, lr_pred).ravel()
    print("tn = {} fp = {} fn = {} tp = {}".format(tn, fp, fn, tp))
    print('RF')
    tn, fp, fn, tp = confusion_matrix(test_y, rf_pred).ravel()
    print("tn = {} fp = {} fn = {} tp = {}".format(tn, fp, fn, tp))
    print('XGB')
    tn, fp, fn, tp = confusion_matrix(test_y, xgb_pred).ravel()
    print("tn = {} fp = {} fn = {} tp = {}".format(tn, fp, fn, tp))
    print('SVM')
    tn, fp, fn, tp = confusion_matrix(test_y, svm_pred).ravel()
    print("tn = {} fp = {} fn = {} tp = {}".format(tn, fp, fn, tp))

    auc_mlp = auc(fpr_mlp, tpr_mlp)
    auc_xgb = auc(fpr_xgb, tpr_xgb)
    auc_rf = auc(fpr_rf, tpr_rf)
    auc_nb = auc(fpr_nb, tpr_nb)

    rcParams['font.family'] = "Times New Roman"
    plt.figure(dpi=100, facecolor='white')
    plt.plot(fpr_mlp, tpr_mlp, label="LR (AUC= %s)" % (round(auc_mlp, 3)))
    plt.plot(fpr_nb, tpr_nb, label="SVM (AUC= %s)" % (round(auc_nb, 3)))
    plt.plot(fpr_rf, tpr_rf, label="RF (AUC= %s)" % (round(auc_rf, 3)))
    plt.plot(fpr_xgb, tpr_xgb, label="XGB (AUC= %s)" % (round(auc_xgb, 3)))
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('1-specificity', fontsize=18)
    plt.ylabel('sensitivity', fontsize=18)
    plt.legend(loc="lower right", fontsize=12)
    plt.show()

    Importance = pd.DataFrame(train_x.columns, columns=['feature'])
    suf = ('_LR', '_RF', '_XGB', '_SVM')

    for i in range(4):
        PI = P_Importance(classlist[i], test_x,
                          test_y).loc[:, ['feature', 'weight']]
        Importance = pd.merge(Importance, PI, on='feature',
                              suffixes=(suf[i-1], suf[i]))

    Importance['meanscore'] = Importance.apply(lambda x: np.mean(x[1:5]), 1)
    ftr_top20 = Importance.sort_values(ascending=False, by='meanscore')[:]

    fig = go.Figure([go.Bar(x=ftr_top20.meanscore, y=ftr_top20.feature, orientation='h', marker={
                    'color': ftr_top20.meanscore, 'colorscale': [[0, '#DDDDDD'], [1.0, 'black']]})])
    fig.update_layout(

        font=dict(
            family='Times New Roman',
            size=18
        ),
        autosize=False,
        width=900,
        height=600,
        #     xaxis=dict(type='category',tickfont_size=15),
        plot_bgcolor='rgba(0,0,0,0)'
    )
    fig.show()


    return Importance

def P_Importance(model, test_x, test_y):  # permutation importance 중요도 뽑기
    perm = PermutationImportance(
        model, scoring="roc_auc", random_state=Ran_state).fit(test_x, test_y)
    ftr_importances = eli5.format_as_dataframe(eli5.explain_weights(
        perm, top=255, feature_names=test_x.columns.tolist()))
    ftr_top = ftr_importances.sort_values(ascending=False, by='feature')[:]
    return ftr_top

ftr_importances = create_curve(classlist, train_x, train_y, test_x, test_y)

data_x

def myRFE(ftr_importances,train_x,train_y,classlist,num=5): #sequential feature selection
    ftr_bot = ftr_importances.sort_values(ascending=False, by='meanscore')
    for j in range(len(ftr_importances)-num):
        best_score = 0
        for i in range(len(train_x.columns)):
            CV_x = train_x.drop(ftr_bot.iloc[i][0],1)
            mlpCV = np.mean(cross_val_score(classlist[0],CV_x,train_y,scoring='roc_auc',cv=cv))
            rfCV = np.mean(cross_val_score(classlist[1],CV_x,train_y,scoring='roc_auc',cv=cv))
            xgbCV = np.mean(cross_val_score(classlist[2],CV_x,train_y,scoring='roc_auc',cv=cv))
            nbCV = np.mean(cross_val_score(classlist[3],CV_x,train_y,scoring='roc_auc',cv=cv))
            Score = np.mean([mlpCV,rfCV,xgbCV,nbCV])
            if Score >= best_score:
                best_score = Score
                print("best_score = ", best_score)
                print("feature = ",ftr_bot.iloc[i][0])
                best_drop = i
        train_x = train_x.drop(ftr_bot.iloc[best_drop][0],1)
        ftr_bot = ftr_bot[ftr_bot['feature']!=ftr_bot.iloc[best_drop][0]]
        print(train_x.columns)
    return train_x.columns

"""위에서 뽑은 importance 이용 Feature selection 과정"""

features = ['Sex', 'Skewness', 'ClusterProminence', 'Coarseness']
#features = myRFE(ftr_importances,train_x,train_y,classlist,num=4)
d_drop = data_x.drop(features, 1)
data_x = data_x.drop(d_drop.columns, 1)
train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.3,stratify=data_y, random_state=Ran_state)
train_x, train_y = TomekLinks().fit_resample(train_x, train_y)

ftr_importances = create_curve(classlist, train_x, train_y, test_x, test_y)
CVROC(classlist,data_x,data_y)