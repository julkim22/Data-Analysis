# -*- coding: utf-8 -*-
"""6. OSA Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJ_u37lP9jreJfJum8rp_uqTX-hbj29h
"""

from google.colab import drive
drive.mount("/content/drive")

pwd

cd /content/drive/MyDrive/Colab/

!pip install eli5

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings(action='ignore')
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score
from sklearn.metrics import precision_recall_fscore_support,roc_curve,auc
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from xgboost import plot_importance
from lightgbm import LGBMClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import *
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import eli5
from eli5.sklearn import PermutationImportance
from imblearn.combine import SMOTETomek
import plotly.graph_objects as go
from sklearn.feature_selection import SelectKBest, chi2

df = pd.read_csv('data/OSA_Data.csv',engine="python")

Ran_state = 80

df.info()

"""데이터 불러오기 끝.
데이터 전처리 시작

X(학습에 이용되는 변수값), Y(라벨, 타겟이 되는 값) 분리.
"""

data_y = df['OSA']

data_x = df.drop('OSA',1)

"""연속형 변수들 MinMaxScaler(변수 데이터를 최솟값이 0, 최댓값이 1의 범위로 표준화) 진행 *continuous한 변수들만."""

DFS = data_x.iloc[:,[0,3,4,53,62,63,65,80]]
data_x = data_x.drop(DFS.columns,1)

MM = MinMaxScaler()
DFSMM = MM.fit_transform(DFS)
DFS = pd.DataFrame(DFSMM,columns=DFS.columns)
data_x = pd.concat([data_x,DFS],1)

"""다중공선성 높은 변수들 제거"""

# from statsmodels.stats.outliers_influence import variance_inflation_factor

# for i in range(len(data_x.columns)):
#         vif = pd.DataFrame()
#         vif["features"] = data_x.columns
#         vif["VIF Factor"] = [variance_inflation_factor(
#             data_x.values, i) for i in range(data_x.shape[1])]
#         vif_remove = vif.sort_values(ascending=False, by='VIF Factor')[:]
#         if vif_remove.iloc[0][1] > 10:
#             data_x = data_x.drop([vif_remove.iloc[0][0]], 1)

#vif_remove

"""학습용 데이터(Train)과 검증용 데이터(Test) 분리.

랜덤하게 선택되어 학습7, 검증3 비율로 분리한다.
"""

train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=0.3,random_state = Ran_state)

test_y.value_counts()

train_x,train_y = SMOTE(random_state=Ran_state).fit_resample(train_x,train_y)
train_x = pd.DataFrame(train_x,columns=data_x.columns)

LR = LogisticRegression(C=10,penalty='l2',random_state=Ran_state)
RF = RandomForestClassifier(max_depth=8,min_samples_split=25,n_estimators=100,random_state=Ran_state)
XGB = XGBClassifier(learning_rate=0.15,max_depth=12,n_estimators=200,random_state=Ran_state)
SVM = SVC(C=10,gamma=1,probability=True,random_state=Ran_state)

classlist = [LR,RF,XGB,SVM]

"""==================================================================

학습 및 예측
"""

def create_curve(classlist,train_x,train_y,test_x,test_y):
    LR = classlist[0]
    RF = classlist[1]
    XGB = classlist[2]
    SVM = classlist[3]

    LR.fit(train_x,train_y)
    RF.fit(train_x,train_y)
    XGB.fit(train_x,train_y)
    SVM.fit(train_x,train_y)

    print('==========================================')
    print(train_x.columns)
    print(LR.coef_)
    print('==========================================')

    lr_proba = LR.predict_proba(test_x)[:,1]
    rf_proba = RF.predict_proba(test_x)[:,1]
    xgb_proba = XGB.predict_proba(test_x)[:,1]
    SVM_proba = SVM.decision_function(test_x)

    fpr_lr,tpr_lr,_=roc_curve(test_y,lr_proba)
    fpr_rf,tpr_rf,_=roc_curve(test_y,rf_proba)
    fpr_xgb,tpr_xgb,_=roc_curve(test_y,xgb_proba)
    fpr_svm,tpr_svm,_ = roc_curve(test_y,SVM_proba)

    auc_lr=auc(fpr_lr, tpr_lr)
    auc_xgb=auc(fpr_xgb, tpr_xgb)
    auc_rf=auc(fpr_rf, tpr_rf)
    auc_svm=auc(fpr_svm, tpr_svm)

    plt.plot(fpr_lr, tpr_lr, label="LR (AUC= %s)"%(round(auc_lr,3)))
    plt.plot(fpr_rf, tpr_rf, label="RF (AUC= %s)"%(round(auc_rf,3)))
    plt.plot(fpr_xgb, tpr_xgb, label="XGB (AUC= %s)"%(round(auc_xgb,3)))
    plt.plot(fpr_svm, tpr_svm, label="SVM (AUC= %s)"%(round(auc_svm,3)))
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('1-specificity',fontsize=12)
    plt.ylabel('sensitivity',fontsize=15)
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right",fontsize=12)
    plt.show()

    Importance = pd.DataFrame(train_x.columns,columns=['feature'])
    suf = ('_LR','_RF','_XGB','_SVM')

    for i in range(4):
        PI = P_Importance(classlist[i],test_x,test_y).loc[:,['feature','weight']]
        Importance = pd.merge(Importance,PI,on='feature',suffixes=(suf[i-1],suf[i]))

    Importance['meanscore'] = Importance.apply(lambda x: np.mean(x[1:5]),1)
    ftr_top20 = Importance.sort_values(ascending=False,by='meanscore')[:]

    fig = go.Figure([go.Bar(x=ftr_top20.meanscore, y=ftr_top20.feature,orientation='h',marker={'color': ftr_top20.meanscore,'colorscale': 'Oryel'})])
    fig.update_layout(
    autosize=False,
    width=500,
    height=800,
    yaxis=dict(type='category',tickfont_size=18)

    )


    fig.show()

    return Importance

def P_Importance(model,test_x,test_y): #permutation importance 중요도 뽑기
    perm = PermutationImportance(model, scoring = "roc_auc", random_state = Ran_state).fit(test_x, test_y)
    ftr_importances = eli5.format_as_dataframe(eli5.explain_weights(perm,top=134, feature_names = test_x.columns.tolist()))
    ftr_top = ftr_importances.sort_values(ascending=False,by='feature')[:]
    return ftr_top

ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)

ftr_importances

def roc_score(classlist,train_x,train_y,test_x,test_y):
    LR = classlist[0]
    RF = classlist[1]
    XGB = classlist[2]
    SVM = classlist[3]

    LR.fit(train_x,train_y)
    RF.fit(train_x,train_y)
    XGB.fit(train_x,train_y)
    SVM.fit(train_x,train_y)

    lr_proba = LR.predict_proba(test_x)[:,1]
    rf_proba = RF.predict_proba(test_x)[:,1]
    xgb_proba = XGB.predict_proba(test_x)[:,1]
    SVM_proba = SVM.decision_function(test_x)

    lr_score = roc_auc_score(test_y, lr_proba)
    rf_score = roc_auc_score(test_y, rf_proba)
    xgb_score = roc_auc_score(test_y, xgb_proba)
    SVM_score = roc_auc_score(test_y, SVM_proba)

    print(lr_score,rf_score,xgb_score,SVM_score)

    Importance = pd.DataFrame(train_x.columns,columns=['feature'])
    suf = ('_LR','_RF','_XGB','_SVM')

    for i in range(4):
        PI = P_Importance(classlist[i],test_x,test_y).loc[:,['feature','weight']]
        Importance = pd.merge(Importance,PI,on='feature',suffixes=(suf[i-1],suf[i]))

    Importance['meanscore'] = Importance.apply(lambda x: np.mean(x[1:5]),1)


    return np.max([lr_score,rf_score,xgb_score,SVM_score]),Importance

def featurefind(ftr_importances,data_x,data_y,classlist):
    best_score = 0
    for i in range(len(ftr_importances.feature)-2):
        ftr_bot = ftr_importances.sort_values(ascending=True,by='meanscore')[:1]
        data_x = data_x.drop(ftr_bot.feature ,1)
        train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=0.3,random_state = Ran_state)
        meanscore,ftr_importances = roc_score(classlist,train_x,train_y,test_x,test_y)
        print(i)
        if meanscore>=best_score:
            best_score = meanscore
            best_bot = data_x
            print("best_score = ",best_score)
    return best_bot

data_x.columns


drop = data_x.drop(['Hypertension', 'Snoring from the BQ', 'Loudness of snoring from the BQ', 'Falling asleep from the BQ','FSS total score', 'Waist circumference', 'Subnasale to stomion'],1)
new_data_x = data_x.drop(drop.columns,1)
train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)
ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)

